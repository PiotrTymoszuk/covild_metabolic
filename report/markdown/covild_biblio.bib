@misc{Ripley2022,
abstract = {Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).},
author = {Ripley, Brian},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{MASS: Support Functions and Datasets for Venables and Ripley's MASS}},
url = {https://cran.r-project.org/package=MASS},
year = {2022}
}
@misc{Schlegel2022,
abstract = {Tests the parallel regression assumption wit the brant test by Brant (1990) <doi:10.2307/2532457> for ordinal logit models generated with the function polr() from the package 'MASS'.},
author = {Schlegel, Benjamin and Steenbergen, Marco},
title = {{brant: Test for Parallel Regression Assumption}},
url = {https://cran.r-project.org/web/packages/brant/index.html},
year = {2022}
}
@article{Brant1990,
abstract = {The proportional odds model for ordinal logistic regression provides a useful extension of the binary logistic model to situations where the response variable takes on values in a set of ordered categories. The model may be represented by a series of logistic regressions for dependent binary variables, with common regression parameters reflecting the proportional odds assumption. Key to the valid application of the model is the assessment of the proportionality assumption. An approach is described arising from comparisons of the separate (correlated) fits to the binary logistic models underlying the overall model. Based on asymptotic distributional results, formal goodness-of-fit measures are constructed to supplement informal comparisons of the different fits. A number of proposals, including application of bootstrap simulation, are discussed and illustrated with a data example.},
author = {Brant, Rollin},
doi = {10.2307/2532457},
issn = {0006341X},
journal = {Biometrics},
month = {dec},
number = {4},
pages = {1171},
pmid = {2085632},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression}},
volume = {46},
year = {1990}
}
@misc{Gagolewski2021,
abstract = {A collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalisation, date-time formatting and parsing, and many more. They are fast, consistent, convenient, and - thanks to 'ICU' (International Components for Unicode) - portable across all locales and platforms.},
author = {Gagolewski, Marek and Tartanus, Bartek},
title = {{CRAN - Package stringi}},
url = {https://cran.r-project.org/web/packages/stringi/index.html},
year = {2021}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
doi = {10.18637/jss.v028.i05},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Model building,NetWorkSpaces,Parallel processing,R,Tuning parameters},
number = {5},
pages = {1--26},
publisher = {American Statistical Association},
title = {{Building predictive models in R using the caret package}},
volume = {28},
year = {2008}
}
@misc{Henry2022,
abstract = {A toolbox for working with base types, core R features like the condition system, and core 'Tidyverse' features like tidy evaluation.},
author = {Henry, Lionel and Wickham, Hadley.},
title = {{rlang: Functions for Base Types and Core R and 'Tidyverse' Features}},
url = {https://cran.r-project.org/web/packages/rlang/index.html},
year = {2022}
}
@misc{Kassambara2021,
author = {Kassambara, Alboukadel},
title = {{rstatix: Pipe-Friendly Framework for Basic Statistical Tests}},
url = {https://cran.r-project.org/package=rstatix},
year = {2021}
}
@book{Wickham2016,
address = {New York},
author = {Wickham, Hadley.},
edition = {1},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {https://ggplot2.tidyverse.org},
year = {2016}
}
@book{Wilke2019,
abstract = {First edition. Intro; Copyright; Table of Contents; Preface; Thoughts on Graphing Software and Figure-Preparation Pipelines; Conventions Used in This Book; Using Code Examples; O'Reilly Online Learning; How to Contact Us; Acknowledgments; Chapter 1. Introduction; Ugly, Bad, and Wrong Figures; Part I. From Data to Visualization; Chapter 2. Visualizing Data: Mapping Data onto Aesthetics; Aesthetics and Types of Data; Scales Map Data Values onto Aesthetics; Chapter 3. Coordinate Systems and Axes; Cartesian Coordinates; Nonlinear Axes; Coordinate Systems with Curved Axes; Chapter 4. Color Scales Color as a Tool to DistinguishColor to Represent Data Values; Color as a Tool to Highlight; Chapter 5. Directory of Visualizations; Amounts; Distributions; Proportions; x-y relationships; Geospatial Data; Uncertainty; Chapter 6. Visualizing Amounts; Bar Plots; Grouped and Stacked Bars; Dot Plots and Heatmaps; Chapter 7. Visualizing Distributions: Histograms and Density Plots; Visualizing a Single Distribution; Visualizing Multiple Distributions at the Same Time; Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots Empirical Cumulative Distribution FunctionsHighly Skewed Distributions; Quantile-Quantile Plots; Chapter 9. Visualizing Many Distributions at Once; Visualizing Distributions Along the Vertical Axis; Visualizing Distributions Along the Horizontal Axis; Chapter 10. Visualizing Proportions; A Case for Pie Charts; A Case for Side-by-Side Bars; A Case for Stacked Bars and Stacked Densities; Visualizing Proportions Separately as Parts of the Total; Chapter 11. Visualizing Nested Proportions; Nested Proportions Gone Wrong; Mosaic Plots and Treemaps; Nested Pies; Parallel Sets Chapter 12. Visualizing Associations Among Two or More Quantitative VariablesScatterplots; Correlograms; Dimension Reduction; Paired Data; Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable; Individual Time Series; Multiple Time Series and Dose-Response Curves; Time Series of Two or More Response Variables; Chapter 14. Visualizing Trends; Smoothing; Showing Trends with a Defined Functional Form; Detrending and Time-Series Decomposition; Chapter 15. Visualizing Geospatial Data; Projections; Layers; Choropleth Mapping; Cartograms Chapter 16. Visualizing UncertaintyFraming Probabilities as Frequencies; Visualizing the Uncertainty of Point Estimates; Visualizing the Uncertainty of Curve Fits; Hypothetical Outcome Plots; Part II. Principles of Figure Design; Chapter 17. The Principle of Proportional Ink; Visualizations Along Linear Axes; Visualizations Along Logarithmic Axes; Direct Area Visualizations; Chapter 18. Handling Overlapping Points; Partial Transparency and Jittering; 2D Histograms; Contour Lines; Chapter 19. Common Pitfalls of Color Use; Encoding Too Much or Irrelevant Information},
address = {Sebastopol},
author = {Wilke, Claus O},
booktitle = {O'Reilly Media},
edition = {1},
isbn = {1492031089},
pages = {389},
publisher = {O'Reilly Media},
title = {{Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures}},
year = {2019}
}
@article{Bates2015,
abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
archivePrefix = {arXiv},
arxivId = {1406.5823},
author = {Bates, Douglas and M{\"{a}}chler, Martin and Bolker, Benjamin M. and Walker, Steven C.},
doi = {10.18637/jss.v067.i01},
eprint = {1406.5823},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bates et al. - 2015 - Fitting linear mixed-effects models using lme4.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Cholesky decomposition,Linear mixed models,Penalized least squares,Sparse matrix methods},
month = {oct},
number = {1},
pages = {1--48},
publisher = {American Statistical Association},
title = {{Fitting linear mixed-effects models using lme4}},
volume = {67},
year = {2015}
}
@article{Robinson2014,
abstract = {The concept of "tidy data" offers a powerful framework for structuring data to ease manipulation, modeling and visualization. However, most R functions, both those built-in and those found in third-party packages, produce output that is not tidy, and that is therefore difficult to reshape, recombine, and otherwise manipulate. Here I introduce the broom package, which turns the output of model objects into tidy data frames that are suited to further analysis, manipulation, and visualization with input-tidy tools. Broom defines the "tidy", "augment" and "glance" generics, which arrange a model into three levels of tidy output respectively: the component level, the observation level, and the model level. I provide examples to demonstrate how these generics work with tidy tools to allow analysis and modeling of data that is divided into subsets, to recombine results from bootstrap replicates, and to perform simulations that investigate the effect of varying input parameters.},
archivePrefix = {arXiv},
arxivId = {1412.3565},
author = {Robinson, David},
eprint = {1412.3565},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robinson - 2014 - broom An R Package for Converting Statistical Analysis Objects Into Tidy Data Frames.pdf:pdf},
keywords = {()},
title = {{broom: An R Package for Converting Statistical Analysis Objects Into Tidy Data Frames}},
url = {http://arxiv.org/abs/1412.3565},
year = {2014}
}
@misc{Xie2022,
abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
author = {Xie, Yihui},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://cran.r-project.org/web/packages/knitr/index.html},
year = {2022}
}
@article{Wickham2019,
abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command. There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor (Gentleman et al., 2004; Huber et al., 2015), which provides an ecosystem of packages that support the analysis of high-throughput genomic data.},
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"{u}}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
doi = {10.21105/joss.01686},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {nov},
number = {43},
pages = {1686},
publisher = {The Open Journal},
title = {{Welcome to the Tidyverse}},
volume = {4},
year = {2019}
}
@article{Luger2022,
author = {Luger, Anna K. and Sonnweber, Thomas and Gruber, Leonhard and Schwabl, Christoph and Cima, Katharina and Tymoszuk, Piotr and Gerstner, Anna K. and Pizzini, Alex and Sahanic, Sabina and Boehm, Anna and Coen, Maximilian and Strolz, Carola J. and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Kirchmair, Rudolf and Feuchtner, Gudrun M. and Prosch, Helmut and Tancevski, Ivan and L{\"{o}}ffler-Ragg, Judith and Widmann, Gerlig},
doi = {10.1148/RADIOL.211670},
issn = {1527-1315},
journal = {Radiology},
keywords = {Anna K Luger,Gerlig Widmann,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Thomas Sonnweber,doi:10.1148/radiol.211670,pmid:35348379},
month = {mar},
pmid = {35348379},
publisher = {Radiology},
title = {{Chest CT of Lung Injury 1 Year after COVID-19 Pneumonia: The CovILD Study}},
url = {https://pubmed.ncbi.nlm.nih.gov/35348379/},
year = {2022}
}
@misc{Gohel2022,
abstract = {Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' documents from 'R Markdown'. Functions are provided to let users create tables, modify and format their content. It also extends package 'officer' that does not contain any feature for customized tabular reporting.},
author = {Gohel, David},
title = {{flextable: Functions for Tabular Reporting}},
url = {https://cran.r-project.org/web/packages/flextable/index.html},
year = {2022}
}
@misc{Allaire2022,
abstract = {Convert R Markdown documents into a variety of formats.},
author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe},
title = {{rmarkdown: Dynamic Documents for R}},
url = {https://cran.r-project.org/web/packages/rmarkdown/index.html},
year = {2022}
}
@book{Xie2016,
abstract = {Bookdown: Authoring Books and Technical Documents with R Markdown presents a much easier way to write books and technical publications than traditional tools such as LaTeX and Word. The bookdown package inherits the simplicity of syntax and flexibility for data analysis from R Markdown, and extends R Markdown for technical writing, so that you can make better use of document elements such as figures, tables, equations, theorems, citations, and references. Similar to LaTeX, you can number and cross-reference these elements with bookdown. Your document can even include live examples so readers can interact with them while reading the book. The book can be rendered to multiple output formats, including LaTeX/PDF, HTML, EPUB, and Word, thus making it easy to put your documents online. The style and theme of these output formats can be customized. We used books and R primarily for examples in this book, but bookdown is not only for books or R. Most features introduced in this book also apply to other types of publications: journal papers, reports, dissertations, course handouts, study notes, and even novels. You do not have to use R, either. Other choices of computing languages include Python, C, C plus plus, SQL, Bash, Stan, JavaScript, and so on, although R is best supported. You can also leave out computing, for example, to write a fiction. This book itself is an example of publishing with bookdown and R Markdown, and its source is fully available on GitHub.},
author = {Xie, Yihui},
isbn = {9781138700109},
pages = {113},
title = {{Bookdown : authoring books and technical documents with R Markdown}},
year = {2016}
}
